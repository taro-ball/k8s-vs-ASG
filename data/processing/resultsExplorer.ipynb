{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#print(Path.cwd().parent.parent.parent)\n",
    "github_repo=\"https://github.com/taro-ball/thesis-results/\"\n",
    "base_path = Path.cwd().parent.parent.parent / \"runs\"\n",
    "#img_path = Path.cwd() / \"img.jpg\"\n",
    "# display(Image(filename=img_path))\n",
    "p = Path(base_path)\n",
    "#p.as_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9649383b64b64722a4f80a11e3a9f18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='2022.0[3-9]', description='Filter'), Checkbox(value=True, description='All'), Dropdâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsts = [\"k8s_taewa_3lite_\",\"asg_taewa_3lite_\", \"k8s_apache_3_\", \"k8s_taewa_3_\", \"asg_apache_3_\", \"asg_taewa_3_\"]\n",
    "tsts = tsts + [\"asg_taewa_3extra_\", \"k8s_taewa_3extra_\", \"asg_raupi_3x_\", \"k8s_raupi_3_\", \"k8s_raupi2_3_\", \"asg_raupi_3\",\"k8s_taewa_6_\", \"asg_taewa_6_\"]\n",
    "tsts = tsts + [\"asg_riwai_3_\", \"k8s_riwai_3_\", \"k8s_taewa2_3_\", \"k8s_apache2_3new\", \"k8s_apache2_3_\", \"asg_raupi_3_70_\"]\n",
    "test = widgets.Dropdown(\n",
    "       options=tsts,\n",
    "       value='k8s_apache_3_',\n",
    "       description='Test:')\n",
    "all = widgets.Checkbox(value=True,\n",
    "       description='All')\n",
    "graphs=[\"QPS_estimatedProcessedBytes.png\",\"QPS_cpuUtilization.png\",\"QPS_groupInServiceCapacity.png\"]\n",
    "date_filter = widgets.Text(\n",
    "       value='2022.0[3-9]', # use glob pattern http://pymotw.com/2/glob/\n",
    "       description='Filter', )\n",
    "box = widgets.HBox([date_filter, all, test ])\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======================================== \n",
      "processing k8s_raupi2_3_\n",
      "\n",
      "c:\\Users\\pa\\OneDrive\\7.UniCode\\0thesis-code\\runs\\2022.05.28_08-55_k8s_raupi2_3_d071\n",
      "[k8s (3)] [ CPU:66.16019970087005 | Dur:331.966207 | qps:3822.269011980263 ]\n"
     ]
    }
   ],
   "source": [
    "#print(test._options_values)\n",
    "tests=[]\n",
    "if all.value:\n",
    "    tests=test._options_values;\n",
    "else:\n",
    "    tests.append(test.value)\n",
    "\n",
    "csv_path=p/'o_summary.csv'\n",
    "\n",
    "# backup old csv in case processing fails\n",
    "if  csv_path.exists():\n",
    "    #Path(csv_path).unlink()\n",
    "    modified=datetime.datetime.fromtimestamp(Path(csv_path).stat().st_mtime).strftime('%m_%d_%Y_%H_%M_%S')\n",
    "    csv_path.rename(p / ('o_summary.csv.'+modified ))\n",
    "\n",
    "# loop through tests\n",
    "for tst in tests:\n",
    "    print('\\n','='*40,f'\\nprocessing {tst}\\n')\n",
    "    search=f'*{date_filter.value}*{tst}*'\n",
    "    nw = datetime.datetime.now()\n",
    "    count=0\n",
    "    iList=[]\n",
    "    #avg_qps_list=[]\n",
    "    #avg_cpu_list=[]\n",
    "    #avg_duration_list=[]\n",
    "    total_err_list=[]\n",
    "\n",
    "    all_qps_list=[]\n",
    "    all_cpu_list=[]\n",
    "    all_duration_list=[]\n",
    "    all_err_list=[]\n",
    "\n",
    "    # loop through each test run\n",
    "    for i in p.glob(search):\n",
    "        print(i)\n",
    "        count+=1\n",
    "        dir = i.name\n",
    "        platform=dir.split(\"_\")[2]\n",
    "        if not (i/\"csv\").exists():\n",
    "            line = f\"<tr><td rowspan='1'><b>No data found, skip: {dir}</b></td></tr>\"\n",
    "            iList.append(line)\n",
    "            continue\n",
    "\n",
    "        #================ read test settings from the log file\n",
    "        log_path = list(i.glob('load*.log'))[0]\n",
    "        max_pods=0\n",
    "        with open(log_path, \"r\") as file:  # the a opens it in append mode\n",
    "            for l in range(100):\n",
    "                statement = next(file).strip()\n",
    "                if m := re.match(\".*scaling_minutes=(\\d+).*\", statement):\n",
    "                    scaling_minutes=int(m.group(1))\n",
    "                elif m := re.match(\".*warmup_max_threads=(\\d+).*\", statement):\n",
    "                    max_threads=int(m.group(1))\n",
    "                elif m := re.match(\".*max_pods=(\\d+).*\", statement):\n",
    "                    max_pods=int(m.group(1))\n",
    "                elif m := re.match(\".*_perc=(\\d+).*\", statement):\n",
    "                    cpu_target=int(m.group(1))\n",
    "                elif m := re.match(\".*warmup_cycle_sec=(\\d+).*\", statement):\n",
    "                    warmup_seconds=int(m.group(1))\n",
    "                    \n",
    "        ##================ get json Fortio data ================\n",
    "        json_paths = (i/\"csv\").glob('*.json')\n",
    "        fortio_path=list(json_paths)[0]\n",
    "\n",
    "        df = pd.read_json(fortio_path, lines=True)\n",
    "        df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "        df.set_index(['StartTime'],inplace=True)\n",
    "\n",
    "        # get performance QPS\n",
    "        perf_df = df[df['Labels'].str.contains('performance-[1-3]', regex=True)]\n",
    "        #perf_data=perf_df.ActualQPS\n",
    "        #qps_lst=list(map(int,perf_data))\n",
    "        qps_lst=list(perf_df.ActualQPS)\n",
    "        avg_qps=np.mean(qps_lst)\n",
    "        all_qps_list.extend(qps_lst)\n",
    "        #avg_qps_list.append(avg_qps)\n",
    "\n",
    "        # get scaling start times\n",
    "        scaling_starts_df = df[df['Labels'].str.contains('scaling-.-1$', regex=True)]\n",
    "\n",
    "        aws_metric0=\"backendConnectionErrors\"\n",
    "        c_path_err=list((i/\"csv\").glob('*'+aws_metric0+'.csv'))[0]\n",
    "        err_df = pd.read_csv(c_path_err, parse_dates=['datetime'], index_col=\"datetime\")\n",
    "\n",
    "        ## ================ PERFORMANCE CloudWatch data ================\n",
    "        aws_metric1=\"cpuUtilization\"\n",
    "        csv_paths = (i/\"csv\").glob('*'+aws_metric1+'.csv')\n",
    "        c_path=list(csv_paths)[0]\n",
    "        mdf = pd.read_csv(c_path, parse_dates=['datetime'], index_col=\"datetime\")\n",
    "        avg_perfomance_cpu_list=[]\n",
    "        #print(c_path)\n",
    "        for index,value in perf_df.iterrows():\n",
    "            perf_start=index#pd.to_datetime(stime)\n",
    "            #print(perf_start)\n",
    "            \n",
    "            # get 4 max:\n",
    "            # mmdf=mdf[perf_start:perf_start+ pd.Timedelta(5, \"m\")]\n",
    "            # mmmdf=mmdf.nlargest(4, aws_metric1).sort_index()\n",
    "            # get 3 mid:\n",
    "            \n",
    "            mmmdf=mdf[perf_start+ pd.Timedelta(1, \"m\"):perf_start + pd.Timedelta(4, \"m\")]\n",
    "            cpu_list=list(mmmdf[aws_metric1])\n",
    "            \n",
    "            average_cpu=np.mean(cpu_list)\n",
    "            avg_perfomance_cpu_list.append(average_cpu)\n",
    "            #print(avg_perfomance_cpu_list)\n",
    "            #print(list(mmmdf.cpuUtilization))\n",
    "            if value.Labels[-1] == '1':\n",
    "                first_perf_start=index\n",
    "                #print(value.Labels)\n",
    "        all_cpu_list.extend(avg_perfomance_cpu_list)\n",
    "        avg_cpu=np.mean(avg_perfomance_cpu_list)\n",
    "        \n",
    "        #avg_cpu_list.append(avg_cpu)\n",
    "\n",
    "        # get number of errors from the start of first performance till the end of last scaling\n",
    "        merr_df=err_df[first_perf_start:first_perf_start + pd.Timedelta(68, \"m\")] \n",
    "        err_list=list(merr_df[aws_metric0])\n",
    "        total_err_list.append(sum(err_list))\n",
    "        #print(total_err_list)\n",
    "\n",
    "        ##================ get json K8s data ================\n",
    "        #print(i)\n",
    "        \n",
    "        max_replicas=99\n",
    "        k8s_depl_path=i/\"k8s-deploy-metrics.json\"\n",
    "        if platform==\"k8s\":\n",
    "            k8s_df = pd.read_json(k8s_depl_path, lines=True)\n",
    "            k8s_df['time'] = pd.to_datetime(k8s_df['time'],format='%d-%m-%Y %H:%M:%S') #26-03-2022 08:44:30\n",
    "            k8s_df.set_index(['time'],inplace=True)\n",
    "            k8s_df=k8s_df.tz_localize(tz='UTC') # localize, Pandas hates tz-naive timestamps\n",
    "            max_replicas=max(k8s_df['replicas'])\n",
    "        else:\n",
    "            max_replicas=int(tst.split(\"_\")[2][0])\n",
    "            max_pods=\"NA\"\n",
    "\n",
    "\n",
    "        setup=f\"cpu={cpu_target} pods={max_pods} treads={max_threads} sc_min={scaling_minutes} warmup={warmup_seconds}\"\n",
    "        ## ================ get json CloudWatch data ================\n",
    "        aws_metric2=\"groupInServiceCapacity\"\n",
    "        csv_paths2 = (i/\"csv\").glob('*'+aws_metric2+'.csv')\n",
    "        c_path2=list(csv_paths2)[0]\n",
    "        cwdf2 = pd.read_csv(c_path2, parse_dates=['datetime'], index_col=\"datetime\")\n",
    "\n",
    "        ## ================ Process scaling data ================\n",
    "        scaling_durations=[]\n",
    "        #print(c_path2)\n",
    "        for index,value in scaling_starts_df.iterrows():\n",
    "            scale_start=index\n",
    "            #print(scale_start)\n",
    "            \n",
    "            #print(ccwdf2)\n",
    "            if platform==\"k8s\":\n",
    "                kk8s_df=k8s_df[scale_start:scale_start+ pd.Timedelta(scaling_minutes, \"m\")]\n",
    "                max_scaled=kk8s_df[kk8s_df['readyReplicas'].ge(max_replicas)]\n",
    "            elif platform==\"asg\":\n",
    "                ccwdf2=cwdf2[scale_start:scale_start+ pd.Timedelta(scaling_minutes, \"m\")] # only need to look 14 min ahead as it's the length of the scaling run\n",
    "                max_scaled=ccwdf2[ccwdf2[aws_metric2].ge(max_replicas)]\n",
    "\n",
    "            # print(ccwdf2.head(6))\n",
    "            # print(max_scaled.head(2))\n",
    "\n",
    "            if max_scaled.size > 0:\n",
    "                cw_reach_max_time=max_scaled.index[0]\n",
    "                duration = cw_reach_max_time - index\n",
    "                duration_in_s = duration.total_seconds()\n",
    "                #minutes = divmod(duration_in_s, 60)[0]\n",
    "                #print(reach_max_time)\n",
    "                #s = scaling_starts_df.index.get_loc(cw_reach_max_time, method='nearest')\n",
    "                #max_minute=scaling_starts_df[reach_max_time]\n",
    "                \n",
    "                # print(index)\n",
    "                # print(cw_reach_max_time)\n",
    "                scaling_durations.append(duration_in_s)\n",
    "            else:\n",
    "                duration_in_s = 0\n",
    "        avg_duration=np.mean(scaling_durations)\n",
    "        all_duration_list.extend(scaling_durations)\n",
    "        \n",
    "        if np.isnan(avg_duration): avg_duration=0\n",
    "        #avg_duration_list.append(avg_duration)\n",
    "\n",
    "        print(f\"[{platform} ({max_replicas})] [ CPU:{avg_cpu} | Dur:{avg_duration} | qps:{avg_qps} ]\")\n",
    "\n",
    "        ##================ generate rows ================\n",
    "        # print(avg_duration_list)\n",
    "        line = \"\"\n",
    "        accent = \"<b>\" if count==1 else \"\"\n",
    "        github_path=f\"{github_repo}tree/master/{dir}\"\n",
    "        line = f\"\"\"<tr><td rowspan='1'>{count}. test id: <b>{dir}</b> <a href='{github_path}'>[raw data]</a></td><td>run settings: {setup}</td><td></td>\n",
    "                <td>{accent}Duration s</td><td>{accent}qps</td><td>{accent}CPU %</td><td>{accent}Err</td>\n",
    "                </tr><tr>\"\"\"\n",
    "        ####====== Images\n",
    "        for y in graphs:\n",
    "            #print(i.name)\n",
    "            img=f'{i.name}/csv/{y}'\n",
    "            line+=f\"<td><img alt='{y}' src='{img}' /></td>\"\n",
    "\n",
    "        ####====== Data\n",
    "\n",
    "        #data=str(lst)[1:-1].replace(\", \", \"<br>\")\n",
    "        line+=f\"<td>{list(map(round,scaling_durations))}<br><b>{avg_duration:.0f}</b></td>\"\n",
    "        line+=f\"<td>{list(map(round,qps_lst))}<br><b>{avg_qps:.0f}</td>\"\n",
    "        line+=f\"<td>{[round(x,2) for x in avg_perfomance_cpu_list]}<br><b>{avg_cpu:.2f}</b></td>\"\n",
    "        line+=f\"<td><b>{total_err_list[-1]}</b></td>\"\n",
    "\n",
    "        line+=\"</tr>\"\n",
    "        #line+=\"</tr><tr><td rowspan='1'>~~~</td></tr>\"\n",
    "        iList.append(line)\n",
    "    \n",
    "    # fix durations list\n",
    "    if not all_duration_list: all_duration_list=[0]\n",
    "\n",
    "    total=f\"\"\"<tr><td>\n",
    "        <h3>{tst} </h3>samples: <b>{count}</b><br>date generated: {nw}<br></td><td></td><td align=\"right\"><h3>Overall:&nbsp;&nbsp;&nbsp;&nbsp;</h3></td>\n",
    "        <td>max:<br>{max(all_duration_list):.0f}<br>min:<br>{min(all_duration_list):.0f}<br>mean:<br><b>{np.mean(all_duration_list):.0f}</b><br></td>\n",
    "        <td>max:<br>{max(all_qps_list):.0f}<br>min:<br>{min(all_qps_list):.0f}<br>mean:<br><b>{np.mean(all_qps_list):.0f}</b><br></td>\n",
    "        <td>max:<br>{max(all_cpu_list):.2f}<br>min:<br>{min(all_cpu_list):.2f}<br>mean:<br><b>{np.mean(all_cpu_list):.2f}</b><br></td>\n",
    "        <td>max:<br>{max(total_err_list):.0f}<br>min:<br>{min(total_err_list):.0f}<br>mean:<br><b>{np.mean(total_err_list):.0f}</b><br></td></tr>\"\"\"\n",
    "    iList.append(total)\n",
    "    iList.insert(0,total)\n",
    "    imagesList = ''.join(iList)\n",
    "\n",
    "    ##================ save HTML ================\n",
    "\n",
    "    html_path=p/f'o_{tst}.html'\n",
    "    header=f\"<!DOCTYPE html><html><head><title>{tst} - Result Explorer</title>\"\n",
    "    header+='<style>table, th, td {border: 1px solid black;font-size: 20px;} body {font: 20px Arial, sans-serif;}</style></head><div style=\"color:red; font-weight: bold\">Please zoom out to see the full table.</div>'\n",
    "\n",
    "\n",
    "        \n",
    "    # header+=\"</tr></table>\"\n",
    "    menuf=p/\"o_menu.html\"\n",
    "    # the menu can be manually customized, will be generated if it does not exist\n",
    "    if not menuf.exists():\n",
    "    #================ generate menu\n",
    "        \n",
    "        menu=''\n",
    "        def sorter(item):\n",
    "            \"\"\"sort by app and nodes\"\"\"\n",
    "            x = ''.join(item.name.split(\"_\")[2:4])\n",
    "            return x\n",
    "\n",
    "        menu+=\"<br><table><tr>\"\n",
    "        files = sorted(p.glob('*.html'), key=sorter)\n",
    "        for h in files:\n",
    "            # if h.name.find(f'{tst}')>0:\n",
    "            #     test_name=f\"<b>{h.stem}</b>\"\n",
    "            # else:\n",
    "            #     test_name=f\"{h.stem}\"\n",
    "            test_name=f\"{h.stem}\"\n",
    "            if (files.index(h)% 2) == 0:\n",
    "                menu+=f\"\\n<td><a href='{h.name}'>{test_name}</a>&nbsp;\"\n",
    "            else:\n",
    "                menu+=f\"<br><a href='{h.name}'>{test_name}</a>&nbsp;</td>\"\n",
    "        menu+=\"</table>\"\n",
    "        print(\"saving menu, exists:\",menuf.exists() )\n",
    "        with open(menuf, 'w') as mf:\n",
    "            mf.write(menu)\n",
    "    ## external menu:\n",
    "    mf = open(menuf, \"r\")\n",
    "    header+=mf.read()\n",
    "    mf.close()\n",
    "    \n",
    "    header+=\"<table>\"\n",
    "    footer=\"</table></html>\"\n",
    "\n",
    "    page= header + imagesList + footer\n",
    "    a = HTML(page)\n",
    "    html_src = a.data\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(html_src)\n",
    "\n",
    "    csv=\"\"\n",
    "    csv+=f'{tst},{count},{tst.split(\"_\")[0]},{tst.split(\"_\")[1]},'\n",
    "    csv+=f'\"{list(map(round,all_duration_list))}\",{max(all_duration_list):.0f},{np.mean(all_duration_list):.0f},{min(all_duration_list):.0f},'\n",
    "    csv+=f'\"{list(map(round,all_qps_list))}\",{max(all_qps_list):.0f},{np.mean(all_qps_list):.0f},{min(all_qps_list):.0f},'\n",
    "    csv+=f'\"{all_cpu_list}\",{max(all_cpu_list):.2f},{np.mean(all_cpu_list):.2f},{min(all_cpu_list):.2f},'\n",
    "    csv+=f'\"{total_err_list}\",{max(total_err_list):.0f},{np.mean(total_err_list):.0f},{min(total_err_list):.0f},\"{setup}\"\\n'\n",
    "\n",
    "    with open(csv_path, \"a\") as csvfile:\n",
    "        csvfile.write(csv)\n",
    "\n",
    "    #display(a, metadata=dict(isolated=True))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d7ec9f48b3cb381e0165fe0ab81a8fedb91dbe2085d19590fb39842e348cce3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
